탐색적 데이터 분석 EDA
    수집한 데이터가 들어왔을 때 이를 다양한 각도에서 관찰하고 이해하는 과정
    데이터를 분석하기 전에 그래프나 통계적인 방법으로 자료를 직관적으로 바라보는 과정
    데이터의 분포 및 값을 검토함으로써 데이터가 표현하는 현상을 더 잘 이해하고 데이터에 대한 잠재적인 문제를 발견 가능
    이를 통해 본격적인 분석에 들어가기 앞서 데이터의 수집 결정 가능
    다양한 각도에서 살펴보는 과정을 통해 문제 정의 단계에서 미쳐 발생하지 못했을 다양한 패턴을 발견하고,
    이를 바탕으로 기존의 가설을 수정하거나 새로운 가설울 세울 수 있다.
    딥러닝에는 데이터가 많이 중요.

    분석 - 가설 생성 - 가설 검증

    분석의 목적과 변수가 무엇이 있는지 확인
        개별 변수의 이름이나 설명을 가지는지 확인 
    데이터를 전체적으로 살펴보기
        데이터에 문제가 없는지 확인
        head나 tail부분 확인, 추가적인 다양한 탐색(이상치, 결측치 등 확인)
    데이터의 개별 속성값을 관찰
        각 속성값이 예측한 범위와 분포를 갖는지 확인. 만약 그렇지 않다면 이유가 무엇인지를 확인.
        속성 간의 관계에 초점을 맞춰 개별 속성관찰에서 찾아내지 못했던 패턴 발견(상관관계, 시각화 등)

EDA는 사람이 한다.
데이터를 알면 알수록 데이터 패턴을 숙지하고 더 성능 좋은 모델을 만들 수 있다.
데이터 간의 상관관계를 착각해 그를 기반으로 학습이 잘 된 모델은 예측 및 검증을 할 땐 성능이 좋아보일 수 있겠지만,
현실에선 전혀 다른 값을 낼 수 있는 것.

성능 최적화
딥러닝은 데이터가 많으면 많을수록 성능이 올라간다. (머신러닝은 한계가 있음)
다양한 패턴을 제공받고 그에 따른 데이터의 순수의미를 학습하므로.

데이터를 사용한 성능 최적화
    데이터를 사용한 성능 최적화 방법은 많은 데이터를 수집하는 것
    데이터 수집이 여의치 않은 상황에서는 임의로 데이터를 생성하는 방법도 고려해볼 수 있음
최대한 많은 데이터 수집하기
    일반적으로 딥러닝이나 머신러닝 알고리즘은 데이터양이 많을수록 성능이 좋음
    가능한 많은 데이터를 수집해야함(빅데이터)
*머신러닝은 돌연변이를 허용하지 않는다.
*딥러닝에선 이상치 또한 의미를 가질 수 있음. (아예 관련 없는 데이터 제외)

데이터 생성하기
    많은 데이터를 수집할 수 없다면 데이터를 만들어 사용할 수 있음
데이터 범위 조정하기(스케일)
    활성화 함수로 sigmoid를 사용한다면 데이터셋 범위를 0~1의 값을 갖도록 하고,
    하이퍼볼릭 탄젠트를 사용한다면 데이터셋 범위를 -1~1의 값을 갖도록 조정할 수 있음
    활성화함수의 선형적 패턴을 이용하겠다(?)

정규화, 규제화, 표준화도 성능 향상에 도움이 됨

알고리즘을 이용한 성능 최적화
    머신러닝, 딥러닝을 위한 알고리즘은 상당히 많음
    수많은 알고리즘 중 우리가 선택한 알고리즘이 최적이 아닐 수 있음
    유사한 용도의 알고리즘을 선택해 모델을 훈련시켜보고 최적의 성능을 보이는 알고리즘을 선택해야 한다.
    머신러닝에서는 데이터 분류를 위해 SVM, KNN 알고리즘들을 선택해 훈련해보거나
    시계열 데이터(연속적인 데이터)의 경우 RNN, LSTM, GRU 등의 알고리즘을 훈련시켜 성능이 가장 좋은 모델을 선택해 사용
    시계열 데이터 - 꼭 연속적인 숫자뿐 아니라 문장, 동영상 등 연속적으로 관계를 가진 데이터

알고리즘 튜닝을 위한 성능 최적화
    성능 최적화를 하는 데에 가장 많은 시간이 소요되는 부분
    모델을 하나 선택해 훈련시키려면 다양한 하이퍼파라미터를 변경하면서 훈련시키고 최적의 성능을 도출해야함.(그리드서치 등)

진단
    성능 향상이 어느 순간 멈추었다면 원인을 분석할 필요가 있음
    문제를 진단하는데 사용할 수 있는 것이 모델에 대한 평가
    
    훈련 성능이 검증보다 눈에 띄게 좋다면 과적합 의심 
        - 해결하기 위해 규제화
    훈련과 검증 결과가 모두 성능이 좋지 않다면 과소적합 의심
        - 과소적합 상황에서는 네트워크 구조를 변경하거나 훈련을 늘리기 위해 에포크 수를 조정
    훈련 성능이 검증을 넘어서는 변곡점이 있다면 조기 종료를 고려

가중치
    가중치에 대한 초기값은 작은 난수를 사용
    작은 난수라는 숫자가 애매하다면 오토인코더 같은 비지도 학습을 이용하여, 
    사전 훈련(가중치 정보를 얻기 위한 사전 훈련)을 진행한 후 지도 학습을 진행하는 것도 방법

학습률
    학습률은 모델의 네트워크 구성에 따라 다르기 때문에 초기에 매우 크거나 작은 임의의 난수를 선택하여 학습 결과를 보고 조금씩 변경해야함
        - 네트워크의 계층이 많다면 학습률은 높아야, 적다면 낮아야 한다.

활성화 함수
    활성화 함수를 변경할 때 손실함수도 함께 변경해야 하는 경우가 많기 때문에 변경은 신중해야 한다
    일반적으로 활성화 함수로 시그모이드나 하이퍼볼릭 탄젠트를 사용했다면 출력층에서는 소프트맥스나 시그모이드 함수를 많이 선택
    주로 입력 - 은닉, 은닉 - 은닉의 레이어에의 활성화함수를 건든다.

배치와 에포크
    일반적으로 큰 에포크와 작은 배치를 사용하는 것이 최근 딥러닝의 트렌드이기는 하지만,
    적절한 배치 크기를 위해 훈련 데이터셋의 크기와 동일하게 하거나 하나의 배치로 훈련을 시켜보는 등 다양한 테스트를 진행해보는 것이 좋다.
    배치 사이즈가 크다는 건 한 번 학습에 데이터 양이 많은 것
    1 에포크 = 배치사이즈로 나뉜 데이터를 전부 한 번 도는 것. 배치사이즈로 한 번 학습할 때마다 가중치가 바뀜
    일반적으로 배치사이즈와 에포크는 반비례하는 형태

옵티마이저 및 손실함수
    일반적으로 옵티마이저는 SGD 확률적 경사하강법을 많이 사용
    네트워크 구성에 따라 차이는 있지만 아담이나 RMSProp등도 좋은 성능을 보임
    다양한 옵티마이저와 손실함수를 적용해보고 성능이 최고인 것을 선택
    
네트워크 구성
    네트워크 구성은 네트워크 토폴로지라고도 함 topology
    최적의 네트워크를 구성하는 것 역시 쉽게 알 수 있는 부분이 아니기 때문에 네트워크 구성을 변경해 가면서 성능을 테스트해야 함
    하나의 은닉층에 뉴런을 여러개 포함시키거나(네트워크가 넓다고 표현),
    네트워크 계층을 늘리되 뉴런 개수는 줄여봄(네트워크가 깊다고 표현)
    혹은 두 가지를 결합하는 방법으로 최적의 네트워크가 무엇인지 확인한 후 사용할 네트워크를 결정해야 함
    
    네트워크가 깊으면 데이터의 순수 의미를 잘 뽑아내겠지만,
    결과가 나와야 역전파로 학습.. 네트워크가 너무 넓거나 깊으면 아웃풋이 없어진다.
    또한 역전파를 거치며 미분되는 가중치가 너무 깊은 네트워크에선 아예 사라져버려 학습이 되지 않는 경우도 있다.

    네트워크가 넓으면 순간적으로 많은 학습, 깊으면 순수 의미를 학습
    얕고 넓은 지식, 좁고 깊은 지식을 가진 사람처럼 생각하면 편하다.

앙상블을 이용한 성능 최적화
    앙상블은 간단히 모델을 두개 이상 섞어서 사용하는 것
    앙상블을 이용하는 것도 성능 향상에 도움이 됨
    알고리즘 튜닝을 위한 성능 최적화 방법은 하이퍼파라미터에 대한 경우의 수를 모두 고려해야하기 때문에 
    모델 훈련이 수십번에서 수백번 필요할 수 있음
    성능 향상은 단시간에 해결되는 것이 아니고 수많은 시행착오를 겪어야 한다.

하이퍼파라미터를 이용한 성능 최적화
    배치 정규화, 드롭아웃, 조기종료

정규화
    데이터 범위를 사용자가 원하는 범위로 제한하는 것
    각 특성 범위를 조정한다는 의미로 특성 스케일링
    순수 의미의 데이터만 전달하겠다.
    편차를 벗어난 데이터는 유니크한(이상치 등) 데이터이므로 제한함

규제화
    모델 복잡도를 줄이기 위해 제약을 두는 방법
    제약은 데이터가 네트워크에 들어가기 전에 필터를 적용한 것
    규제를 이용하여 모델 복잡도를 줄이는 방법
    드롭아웃, 조기 종료
    뉴런의 개수가 많아지면 모델이 복잡해진다.
    조기종료로 학습을 그만두고 모델을 바꾸지 않겠다.

표준화
    기존 데이터를 평균은 0, 표준편차는 1인 형태의 데이터로 만드는 방법
    표준화 스칼라 혹은 z스코어 정규화
    즉 데이터의 값의 범위를 정함.(정규화는 데이터의 분포(?)의 범위를 정함.) -?

배치 정규화
    데이터 분포가 안정되어 학습 속도(?)를 높일 수 있음
    배치 정규화는 기울기 소멸이나 기울기 폭발 같은 문제를 해결하기 위한 방법.
        -기울기 소멸: 안장점 생각하면 될듯
        -기울기 폭발: 발산해버림
    일반적으로 기울기 소멸이나 폭발 문제를 해결하기 위해 손실 함수로 relu를 사용하거나 초기값 튜닝, 학습률 등을 조정
    단계마다 활성화 함수를 거치면서 데이터셋 분포가 일정해지기 때문에 속도를 향상
    편향을 막겠다..

기울기 소멸: 오차 정보를 역전파시키는 과정에서 기울기가 급격히 0에 가까워져 학습이 되지 않는 현상
기울기 폭발: 학습 과정에서 기울기가 급격히 커지는 현상
기울기 소멸과 폭발 원인
    내부 공변량 변화 
        - 공통적인 요소의 변화량. 공통적인 분모가 너무 많으면 편향되었다는 뜻..
        - 공변량이 커지면 기울기가 가파라지고 작아지면 기울기가 소멸될 수 있다(?)
    네트워크의 각 층마다 활성화 함수가 적용되면서 입력 값들의 분포가 계속 바뀌는 현상

배치 정규화
    - 분산된 분포를 정규분포로 만들기 위해 표준화와 유사한 방식을 미니배치에 적용하여 평균은 0, 표준편차는 1로 유지하도록 함
    1. 미니 배치 평균을 구함
    2. 미니 배치의 분산과 표준편차를 구함
    3. 정규화를 수행
    4. 스케일을 조정함 (데이터 분포 조정)

배치 정규화 단점
    배치 크기가 작을 때는 정규화 값이 기존 값과 다른 방향으로 훈련될 수 있음
    RNN(순환신경망)은 네트워크 계층별로 미니 정규화를 적용해야 하기 때문에 모델이 더 복잡해지면서 비효율적
    문제를 해결하기 위한 가중치 수정, 네트워크 구성 변경 등을 수행
    배치 정규화를 적용하면 적용하지 않았을 때보다 성능이 좋아지기 때문에 많이 사용
    신경망의 층이 깊어질수록 학습할 때 가정했던 입력분포가 변화하여 엉뚱한 학습이 진행될 수 있음
    배치 정규화를 적용해서 입력 분포를 고르게 맞추어 주면서 과적합을 해소

    배치 정규화로 오히려 편향된 데이터로 학습할 수 있음.
    또한 순환신경망에서 모델이 너무 복잡해져 효율이 떨어질 수 있다.

드롭아웃을 이용한 성능 최적화
    훈련 데이터셋에 대해 훈련을 계속한다면 오류는 줄어들지만 검증 데이터셋에서는 그렇지 않게 된다.
    드롭아웃
        훈련할 때 일정 비율의 뉴런만 사용하고 나머지 뉴런에 해당하는 가중치는 업데이트하지 않는 방법
        매 단계마다 사용하지 않는 뉴런을 바꾸어가며 훈련시킴
        노드를 임의로 끄면서 학습하는 방법으로, 은닉층에 배치된 노드 중 일부를 임의로 끄면서 학습
        꺼진 노드는 신호를 전달하지 않음.
        너무 과도하게 사용하면 언더피팅이 되므로 적절히 사용해야 한다.
    