개발자, 코더는 데이터를 분류할 때 쓰이는 통계학, 선형대수학 등 해당 분야에 대해 사실상 석박사 수준의 해석을 할 필요가 있을까?
공식을 직접 전개해가며 문제를 푸는 일은 컴퓨터에게 맡기면 된다.
그러나 공식을 인지는 해야한다. 하이퍼 파라미터를 직접적으로 입력해주는 경우가 많기 때문에 돌아가는 원리를 알아두면 좋은 것이다.
수학 또한 하나의 언어이다. 개발자끼리, 개발자와 데이터, 컴퓨터 간의 소통을 위해 알아야 하는 언어인 것이다.


나이브 베이지안 분류기

연속형 값의 확률   
    해당 데이터를 적절히 표현하는 함수를 생성한 후 해당 함수의 적분 값을 취한다.
    특정 위치의 값은 없고 적분 값을 취해 구간의 확률을 계산.

확률의 기본 성질
    확률은 모든 사건에 대해 반드시 0에서 1 사이의 값을 가진다. (0 <= P(X) <= 1)
    각 사건들이 서로 관계가 없는 경우, 즉 각 사건들이 일어날 확률이 다른 사건에 영향을 주지 않는 경우는 독립적이라 함.
    조건부 확률: 어떤 사건이 일어난다고 가정했을 때 다른 사건이 일어날 확률.
    P(A|B)
        B라는 사건이 발생했을 때 A와 B 사건의 교집합이 발생할 확률
        P(A|B) = (P(A&B)/P(B))
        A={x|x는 홀수}, B={x|x는 4보다 작은 수}, x > 0
        해당 사건의 (A|B)는 B의 상황에서 A를 만족하는 값을 찾으면 되므로 {1,3}
        그럼 P(A|B)는 2/3이 된다. (X) 확률로 계산을 안했다. 근데 분모가 상쇄되므로 맞는듯?
베이즈 정리
    두 확률 변수의 사전확률과 사후확률 사이의 관계를 나타내는 정리
베이즈 정리의 전제
    객관적인 확률이 존재하지 않고 이전 사건으로 인해 확률이 지속적으로 업데이트 된다.
    베이즈주의자는 실제로 뒤집어 카드가 나온 확률을 기반으로 실행할 때마다 계속하여 확률들을 업데이트
    빈도주의자는 다음 확률을 계속하여 실행할 경우 1/3에 수렴할 것이므로 1/3으로 간주(카드 뒤집기)
H는 가설, D는 데이터일 때 P(H|D)는 사후확률
사후확률: 데이터가 주어졌을 때 해당 가설이 맞는지에 대한 확률
P(D|H)는 가설이 주어졌을 때 해당 데이터가 존재할 확률

검은색 작은 공 = 6, 흰색 작은 공 = 3, 검은색 큰 공 = 1, 흰색 큰 공 = 3
크기와 색깔이 다른 13개의 공 중 1개의 공을 무작위로 뽑을 때 뽑은 공이 큰 공이었다면 이 공이 검은색일 확률은?
큰공 = P(BIG), 작은공 = P(SMALL), 검은공 = P(BLACK), 흰공 = P(WHITE)
큰 공을 뽑을 때 검은색일 확률 P(BLACK|BIG)
P(BLACK|BIG) = (P(BLACK)P(BIG|BLACK)) / P(BIG)

나이브 베이지안 분류기
    ex)
    하나의 문장이 있을 때 이 문장을 soprts와 not soprts로 나누는 분류기 만들기
BoW(Bag of Words)
    단어별로 인덱스가 부여되어 있을 때 한 문장 또는 한 문서에 대한 벡터를 표현하는 기범
        하나의 단어를 벡터화시킬 때는 원핫인코딩 기법 사용

의사결정 트리
    어떤 규칙을 하나의 트리 형태로 구현 후 분류, 회귀 문제를 해결
    규칙은 if else문으로 표현 가능
    트리는 일종의 경로를 표현하는 것
    트리 구조의 마지막 노드에는 분류문제에선 클래스, 회귀 문제에선 예측치가 들어간다.

의사결정트리는 딥러닝 기반을 제외한 전통적인 통계 기반의 머신러닝 모델 중 효과와 실용성이 가장 좋음
    테이블형 데이터에 있어 설명력, 성능의 측면에서 딥러닝 모델들과 대등하게 경쟁
    앙상블 모델, 부스팅 같은 새로운 기법이 모델의 성능을 향상

분할 속성
    부모 노드에 들어가는 if else문의 조건
    어떤 분할 속성이 가장 모호성을 줄일 것인지 파악

재귀적 지역 최적화 방법: 첫 문제로 분할 속성 설정, 다음 데이터 속에서 최적의 분할 속성을 찾아냄
    분할 속성이 잘못 설정되면 트리의 성능이 떨어진다. 가장 큰 영향을 주는 조건을 최상단에 걸어 준다.

엔트로피
    어떤 목적 달성을 위한 경우의 수를 정량적으로 표편하는 수치
    현재 정보 제공 상태를 측정
    정보를 제공하는 기준값 정하고 그 값을 최소화 또는 최대화하는 방향으로 알고리즘 실행

낮은 엔트로피 = 경우의 수가 적음 = 낮은 불확실성
높은 엔트로피 = 경우의 수가 높음 = 높은 불확실성

엔트로피 측정 방법
    샤논.

로지스틱 회귀 분류는 연속적인 공식을 기반해 분류,
의사결정 트리는 범주를 나눠 그 조건에 기반해 분류

정보 이득
    엔트로피를 사용하여 속성별 분류 시 데이터가 얼마나 순수한지를 측정하는 지표
    각 속성을 기준으로 데이터를 분류했을 때 엔트로피를 측정
    전체 엔트로피 - 속성별 엔트로피 = 속성별 정보이득
    
    속성별 엔트로피: 속성으로 데이터를 분류했을 때 그 속성이 가진 모든 클래스의 각 엔트로피 계산 후 데이터의 개수만큼 가중치를 줌
    정보이득이 크면 클수록 그 속성을 기준으로 데이터를 분류했을 때 얻을 수 있는 정보량이 많다는 뜻.
    한 속성을 기준으로 데이터를 나눌 때 엔트로피가 작다면 해당 속성을 기준으로 데이터를 나누기 좋다고 할 수 있다.

의사결정트리 특징
    -재귀적 작동
        가지가 되는 속성을 선택한 후 해당 가지로 데이터를 나누면 이전에 적용되었던 알고리즘이 적용된다.
    - 속성 기준으로 가지치기 수행
        가장 불확실성이 적은 속성을 기준으로 가지치기 수행
    - 중요한 속성 정보 제공
        처음 분리 대상이 되는 속성이 가장 중요한 속성
         처음부터 최대한 불확실성을 줄여야 나머지도 나누기 쉬워진다.
         이 특징을 '해석 가능한 머신러닝'이라고 부름

의사결정트리 장점
    - 불필요한 속성 값에 대한 스케일링
        전처리 단계 없이 바로 사용할 수 있다.
    -강건한 이상치
        관측치의 절대값이 아닌 순서가 중요하기 때문에 필요 이상으로 엄청 큰 값이나 작은 값에 대해서도 분류 성능이 크게 덜어지지 ㅏㅇㄶ는다.
    - 자동적인 변수 선택
        알고리즘에 의해 중요한 변수가 우선적 선택, 좀 더 손쉽게 중요한 속성을 확인할 수 있다.
        의사결정트리 계열의 알고리즘이 가진 가장 큰 장점 중 하나.

정보 이득의 문제점
    수식의 특성상 속성의 값이 다양할수록 선택의 확률이 높아지는 문제가 발생한다.
    데이터가 매우 많고 속성이 다양할 때 구분을 할 대상의 숫자값이 줄어든다.
    해당 속성의 엔트로피 값이 낮아져서 단순히 속성 안에 있는 값의 종류를 늘리는 것만으로도 정보 이득이 높아진다.

C4.5
    정보 이득을 측정하는 방식을 좀 더 평준화시켜 단순한 정보 값을 대신 사용
    기존 정보 이득의 분모에 평준화 함수 SplitInfo 추가
    정교한 불순도 지표 이용
    범주형 변수 뿐 아니라 연속형 변수를 사용 가능
    결측치가 포함된 데이터도 사용 가능
    과적합을 방지하기 위한 가지치기

지니 지수
    경제학에서 소득의 불평등도를 측정할 때 사용하는 지표
    의사결정트리에서 각 속성의 불순도를 측정하는 방법으로 사용
    엔트로피 대신 지니지수를 이용, 이진분할한다고 생각하면 될 것 같다.

트리 가지치기
    클래스의 마지막 노드인 잎노드의 개수를 개발자가 직접 결정
        1개로 이루어진 잎 노드가 많을 경우 과대적합이 되어있는 상태
        잎 노드의 개수와 관계 없이 해당 가지에 불확실성이 너무 높을 경우 의사결정트리의 성능에 문제를 줄 수 있음
    의사결정트리의 마지막 노드의 개수를 지정하여 트리의 깊이를 조정하는 방법
    사전 가지치기(오버피팅이 될 것 같아 미리 설정)
        처음 트리를 만들 때 트리의 깊이, 마지막 노드의 최소 개수 등 사전에 결정하여 입력
        데이터 분석가가 하이퍼 매개변수로 모든 값을 입력해야 하는 어려움이 있다.
        계산 효율이 좋고 작은 데이터셋에서도 쉽게 작동한다.
        사용자가 중요한 속성 값을 놓치거나 과소적합 문제 발생할 수 있다.
    사후 가지치기(오버피팅이 되어 설정)
        트리를 먼저 생성한 후 실험적으로 하이퍼 매개변수를 조정
        하나의 지표를 정해두고 실험적으로 다양한 하이퍼 매개변수를 조정하며 최적의 값을 찾음
        최종 노드의 개수, 트리의 깊이, 선택되는 속성의 개수 등을 하이퍼 매개변수로 보고 조정하며 성능 비교
        데이터를 훈련, 검증, 테스트셋으로 분류, 훈련과 테스트셋의 성능 비교
        그리드 서치.

연속형 데이터를 나누는 기준
    모든 데이터를 기준점으로 데이터를 나누기
    통계적 수치로 중위값이나 4분위수를 기준점으로 나누기
    Y클래스의 값을 기준으로 해당 값이 변할 떄를 기준으로 나눈다.
        데이터를 정렬 후 y클래스 값이 변할 때 그 경계. 앞뒤 데이터의 중간값

앙상블
    대중적인 데이터 분석 알고리즘   
    최근 머신러닝/딥러닝 분야에서 딥러닝 다음으로 부스팅 알고리즘이 핵심적으로 사용됨
    선형회귀나 로지스틱 회귀는 가장 대중적인 알고리즘이고, 그 다음이 의사결정트리와 앙상블 계열 알고리즘, 딥러닝
    여러개의 알고리즘들이 하나의 값을 예측하는 기법을 통칭하여 말함
    시간이 아주 오래 걸리지만 좋은 성능을 낸다

    기법
     - 바닐라 앙상블
        가장 기본적인 앙상블 기법. 아무것도 처리하지 않은 앙상블 모델을 의미
        일반적으로 가중치 평균이나 투표 방식으로 만들어지는 앙상블 모델
     - 부스팅
        하나의 모델에서 여러 데이터를 샘플링한 다음 그 샘플링된 데이터로 각각의 모델을 만드는 기법
     - 배깅
        'boosting aggregation(부스팅 집합)'의 줄임말로 부스팅을 좀 더 발전시킨 기법
