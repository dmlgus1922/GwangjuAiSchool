데이터 전처리

연속형 데이터를 범주형 데이터로 변환하여 처리하기
 바인딩
 함수 cut 사용.
 bins 리스트에 구간의 시작값, 끝값을 넣고 구간의 이름을 리스트로 나열
 cut함수로 나눌 시리즈 객체와 구간, 구간의 이름을 넣어주면 해당 값을 바인딩하여 표시해줌.

데이터 크기 맞추기
 -피쳐 스케일링
    스케일링:
     데이터 간의 범위를 맞춤
      몸무게와 키를 하나의 모델에 넣으면 데이터의 범위가 훨씬 넓어져 키가 몸무게에 비해 모델에 과다하게 영향을 줌
     x1과 x2의 변수 범위가 다를 때 하나의 변수 범위로 통일시켜 처리. 비율화하는 것.
     스케일링할 때는 브로드캐스팅 개념으로 스칼라값과 벡터값 간 연산
     - 최솟값-최댓값 정규화:
        최솟값과 최댓값을 기준으로 0에서 1, 또는 0에서 지정 값까지로 값의 크기를 변화시킨다.
        각 데이터를 같은 피쳐 최솟값으로 빼주고 (최댓값 - 최솟값)으로 나눠준다.
     - z스코어
        기존 값을 푯준 정규분포값으로 변환하여 처리
        각 값에 열의 평균값을 빼고 표준편차로 나눠준다.

* 학습이 된 모델 = 분류기
    
머신러닝 프로세스와 데이터 전처리
 데이터를 확보한 후 데이터를 정제 및 전처리
 학습용(트레이닝 데이터)과 테스트 데이터를 나눠 학습용 데이터로 학습을 실시
  +추가로 학습이 잘 되는지 검증 데이터를 사용해 확인 
 학습 결과를 평가 지표와 비교하여 하이퍼 매개변수 변환
 최종적인 모델 생성하여 테스트 데이터셋으로 성능을 측정
 모델을 시스템에 배치하여 모델을 작동시킴

* 원핫 인코딩으로 피쳐를 펼치면 그만큼 데이터테이블의 차원이 넓어진다.
    학습에는 어느정도는 도움이 되겠지만 너무 큰 차원의 증폭은 오히려 성능을 떨어트린다.
    의미 없는 데이터 또한 많아질 수도 있다.
    또한 오버피팅될 가능성도 높다.

선형회귀: 회귀는 회귀인데 선형으로 회귀하겠다. 줄을 하나 그어버리겠다.
    종속변수 y와 한 개 이상의 독립변수 x와의 선형 상관관계를 모델링하는 회귀분석 기법
    기존 데이터를 활용해 연속형 변수값을 예측
    y = ax + b 꼴의 수식을 만들고 a와 b의 값을 찾아냄
    하나 이상의 특성(피쳐)과 연속적인 타깃 변수 사이의 관계를 모델링하는 것이 목적
    지도 학습의 회귀는 범주형 클래스 레이블이 아니라 연속적인 출력 값을 예측
    -단순 선형 회귀
      단순 선형 회귀는 하나의 특성(설명 변수x)과 연속적인 타깃(응답 변수 y) 사이의 관계를 모델링
      w0는 y축 절편을 나타내고 w1은 특성의 가중치(기울기)를 나타냄
       학습을 한다는 것은 이 w값을 결정하는 것.
      특성과 타깃 사이의 관계를 나타내는 선형 방정식의 가중치를 학습하는 것이 목적
      이 방정식으로 훈련 데이터셋이 아닌 새로운 샘플의 타깃 값을 예측할 수 있음
      하이퍼 파라미터: 학습 초기에 주어지는 사람이 입력하는 w0, w1.
      데이터에 가장 잘 맞는 이런 직선을 회귀직선이로고도 한다.
      회귀직선과 훈련 샘플 사이의 거리를 오프셋(offset) 또는 예측 오차인 잔차(residual)라고 함
      
      예측 함수와 실제값 간의 차원이
        예측 함수는 예측값과 실제값 간의 차이를 최소화하는 방향으로
        데이터 n개 중 i번째 데이터의 y값에 대한 실제값과 예측값의 차이
        오차 값들이 음수와 양수로 나왔을 때 값들 간의 차이가 상쇄되어 0으로 계산될 수 있다.
        때문에 잔차를 제곱해 더하는 방법이 있다.(제곱오차)
        제곱오차를 최소화하는 w0와 w1을 찾아야 한다.

      비용 함수의 개념
        -머신러닝에서 최소화해야 할 예측값과 실제값의 차이
        -가설함수: 예측값을 예측하는 함수 (f(x) = h세타(x))
        -함수의 입력값은 x이고 함수에서 결정할 것은 세타로, 가중치 값인 Wn(기울기, 절편)
      비용함수가 두 개의 가중치 값을 결정된다.
        잔차의 제곱합: 예측값인 가설함수와 실제값인 y값 간의 차이를 제곱해서 모두 합함.
            총 데이터는 m개가 존재하고 각 데이터의 예측값과 실제값을 뺀 후 제곱한 값들을 모두 합한 값
        손실함수: 비용함수에서 잔차의 제곱합 부분
        평균 제곱 오차: 잔차의 제곱합을 2m으로 나눈 값
    
      비용함수의 편미분
         