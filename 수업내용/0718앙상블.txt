앙상블

대중적인 데이터 분석 알고리즘
최근 머신러닝/딥러닝 분야에서 딥러닝 다음으로 부스팅 알고리즘이 핵심적으로 사용됨
선형회귀나 로지스틱회귀는 가장 대중적인 알고리즘. 그 다음 앙상블

앙상블
    여러 개의 알고리즘들이 하나의 값을 예측하는 기법을 통칭
    회귀문제에서는 가중 평균이나 단순 평균을 구하는 방식으로 Y값을 예측
    메타 분류기라고도 부름
        메타는 일종의 상위 또는 추상화라는 개념
        여러 분류기들을 모아 하나의 분류기를 만들어 이를 메타 분류기라고 한다.
    시간이 굉장히 오래 걸리지만 비교적 좋은 성능을 냄
    하나의 데이터를 넣음 - 여러 모델에 학습 - 테스트 데이터를 각 모델에 입력 - 투표 또는 여러 가중치 기법을 적용하여 선택

바닐라 앙상블
부스팅: 하나의 모델에서 여러 데이터를 샘플링한 다음 그 샘플링된 데이터로 각각의 모델을 만드는 기법
배깅: 부스팅 집합의 줄임말로 부스팅을 좀 더 발전시킨 기법, 부스팅으로 결과를 뽑은 것들을 모은 것.

바닐라 앙상블 - 투표 분류기: '여러개의 모델'을 만들어 모두 같은 데이터를 넣고 결과를 취합해 가장 많이 선택된 결과를 취한다.
    앙상블의 가장 기본적 형태
    다수결 분류기라고도 함
    각 분류기마다 가중치를 주고 해당 가중치를 각 모델에 곱해 가중치의 합을 구하는 방식
    장점: 다양한 모델을 만든 후 다음 단계로 매우 쉽게 만들 수 있음

배깅 - 병렬
    하나의 데이터셋에서 샘플링을 통해 '여러 개의 데이터셋'을 만든 다음 각 데이터셋마다 모델을 개발하여 투표 분류기로 만드는 기법
    단순하면서 성능이 높아 특히 트리 계열 알고리즘과 함께 많이 사용되며 통계적인 샘플링 기법이나 딥러닝 기법과도 함꼐 사용
    샘플링: 다루고자 하는 데이터가 전채 모수라면 그 모수에서 일부분을 뽑아서 데이터를 분석
    배깅의 장점: 다양한 데이터셋에서 강건한 모델을 개발할 수 있다.
    각각의 개별 모델은 부스팅이라 볼 수 있다. 부스팅 기법을 통해 만들어진 모델을 모을 수도 있다.
    *학습의 단순화: 데이터를 자르면 학습하기가 쉬울 수 있다.
    *데이터의 단순화 작업: 각 모델마다 영역을 쪼개 학습할 때 공통적으로 겹치는 부분을 쉽게(?) 학습할 수 있다(?)
        즉 모델이 하나만 있다면 데이터를 나누기 힘들겠지만 여러 모델이 겹치는 부분을 찾으므로 데이터의 형상을 단순화시킨다.

    약분류기, 강분류기
    배깅 기법은 여러개의 약분류기(대략적 분류), 강분류기를 만드는 것이다.
    약분류기는 기본적으로 과소적합이 다소 있지만 과적합되어 있지 않은 모델. 다소 느슨하게 경계를 생성하는 여러개의 약한 분류기를 앙상블한다면 좀 더 정확한 경계를 생성한다.
    각각의 작은 데이터로는 모든 구체적인 분류 영역을 정할 수 없지만 많은 데이터로 투표한다면 더 높은 성능을 기대할 수 있다.

    부트스트래핑: 모수 데이터로부터 학습 데이터를 추출할 때 임의의 데이터를 추출한 후 복원추출하는 여러번의 과정
    복원추출: 전체 데이터 중 일부룰 추출해 데이터셋을 만들고 다시 그 데이터를 모수에 집어넣어 새로 추출해 데이터셋을 또 만드는 방식
    .632 부트스트래핑 기법: 계속 추출할 수록 0.632퍼에 가까워짐

    배깅은 bootstrap aggregation의 약자. 부트스트랩 연산의 집합이라는 개념
    데이터셋으로부터 부분집합 n개를 추출 -> 앙상블 방법과 달리 하나의 모델에 다양한 데이터셋을 넣어서 n개의 모델을 생성
    높은 분산으로 일반적인 모델로 만들 경우 과적합이 심한 데이터셋에 좀 더 강건
    각 모델들은 해당 데이터셋에 맞춰진 과적합 모델. 각각의 모델마다 오버피팅된 영역이 다름. 이를 통합해 입력한 데이터마다 최적화된 값을 낸다?

    out of bag Error (oob)

랜덤포레스트: 하나의 모델을 나무라고 한다면 이러한 나무들을 이용해 랜덤하게 데이터를 뽑아서 숲을 생성하는 알고리즘
    배깅알고리즘을 의사결정트리에 적용한 모델

배깅알고리즘 하이퍼파라미터
base_estimator 사용될 수 있는 모델,
n_estimators subset으로 생성되는 모델의 개수,
max_samples 최대데이터 개수 또는 비율,
max_features 최대 사용 피쳐 또는 비율,
bootstrap 부스트스트랩 사용 여부,
oob_score oob score 산출 여부, 
warm_start 이전에 학습된 모델을 사용할 것인가에 대한 정보,

부스팅 - 직렬
학습 라운드를 차례로 진행하면서 각 예측이 틀린 데이터(오차)에 점점 가중치를 주는 방식.
라운드별로 잘못 분류된 데이터를 좀 더 잘 분류하는 모델로 만들어 최종적으로 모델들의 앙상블을 만드는 방식
배깅 알고리즘이 처음 성능을 측정하기 위한 기준

배깅과 부스팅의 차이
 병렬화 가능 여부: 배깅(병렬화), 부스팅(직렬화, 전 단계의 분류기에 의해 다음 단계 분류기가 결정되기 때문에)
 작은 분류기를 만드는 방법이 다름
 기준 추정치
 성능 차이

병렬화 가능 여부
    배깅은 데이터가 n개라면 n개의 CPU로 한번에 처리하도록 구조를 설계할 수 있음.
    배깅은 데이터를 나눠 데이터마다 조금씩 다른 모델을 생성
    부스팅은 단계적으로 모델들을 생성하고 해당 모델들의 성능을 측정한 후 다음 단계로 넘어가 병렬화를 지원하지 않음
    부스팅은 배깅에 비해 속도가 매우 떨어짐

기준 추정치
    배깅 개별 모델들은 높은 과대적합으로 모댈의 분산이 높음
    부스팅은 각각의 모델에 편향이 높은 기준추정치를 사용하여 개별 모델들은 과소적합이 발생하지만 전체적으로 높은 성능을 낼 수 있는 방향으로 학습
    부스팅 모델의 이러한 특징을 약한 학습자라고 부름

성능 차이
    부스팅은 기본적으로 비용(속도, 시간 등)이 높은 알고리즘
    배깅은 데이터의 부분집합에 대해 학습을 수행하기 때문에 부스팅보다 좋은 성능을 내기는 어려움
    초기 성능을 측정할 때는 배깅, 이후의 성능 측정은 부스팅으로 하는 것이 일반적인 접근

에이다 부스트
    매 라운드마다 인스턴스, 즉 개별 데이터의 가중치를 계산하는 방식

스텀프
    그루터기라는 뜻으로 잘린 나무의 밑부분
    에이다 부스트에서 스텀프는 학습할 때 큰 나무를 사용하여 학습하는 것이 아니라 나무의 그루터기만을 사용하여 학습한다는 개념
    1뎁스 또는 2뎁스 정도의 매우 간단한 모델을 여러개 만들어 학습, 해당 모델들의 성능을 에이다부스트 알고리즘을 적용하여 학습하는 형태

오차를 줄이기 목표. 분류든 회귀든
-가중치를 갱신. 그라디언트 디센트 이용.
이 학습을 부스팅 기법에 적용
의사결정트리 데이터 입력 지니계수 기반 분류 참 거짓
학습의 단계 그라디언트디센트 스텝을  줄이는 만큼 오차 

랜덤포레스트, 에이다부스트, 그라디언트부스팅, 히스토그램부스팅, 엑스트라트리



k-평균 군집화

knn알고리즘
    모든 기계학습 알고리즘 중에서도 가장 간단하고 이해하기 쉬운 분류 알고리즘
    가까이 있는 것들에 포함
    지도학습에 가깝다

군집
    비슷한 샘플을 클러스터로 모음
    비슷한 샘플을 구별해 하나의 클러스터 또는 비슷한 샘플의 그룹으로 할당하는 작업. 군집은 다양한 애플리케이션에서 사용
    데이터 분석, 고객 분류, 추천 시스템, 검색 엔진, 이미지 분할, 준지도 학습, 차원 축소 등에 사용

이상치 탐지
    정상 데이터가 어떻게 보이는지를 학습하고 비정상 샘플을 감지하는데 사용
밀도 추정
    데이터셋 생성 확률 과정의 확률 밀도 함수를 추정한다 probability density function PDF
    밀도 추정은 이상치 탐지에 널리 사용된다.

k-means알고리즘
    주어진 n개의 관측값을 k개의 클러스터로 분할하는 알고리즘으로, 관측값들은 거리가 최소인 클러스터로 분류된다.
    k평균 방식
        가장 대표적인 비지도학습 알고리즘
        k평균 클러스터 방식
        출력 데이터 없이 입력만을 가지고 학습하여 결과(라벨)를 생성
    입력값: k->클러스터 수, D->n개의 데이터
    출력값: k개의 클러스터
    반복 몇 번으로 레이블이 없는 데이터셋을 빠르고 효율적으로 클러스터로 묶는 간단한 알고리즘
    프로토타입 기반 군집에 속함
    k-평균 알고리즘이 원형 클러스터를 구분하는 데 뛰어나지만, 이 알고리즘 단점은 사전에 클러스터 개수 k를 지정해야 한다는 것
    적절하지 않은 k를 고르면 군집 성능이 좋지 않음
    나중에 군집 품질을 평가하는 기법 엘보우 방법과 실루엣 그래프
    프로토타입 기반 군집
        각 클러스터가 하나의 프로토타입으로 표현된다는 뜻
        연속적인 특성에서는 비슷한 데이터 포인트의 센트로이드(centroid 평균)이거나
        범주형 특성에서는 메도이드(medoid 가장 대표되는 포인트나 가장 자주 등장하는 포인트)가 됨
    
    엘보우 방법에서는 k를 1부터 증가시키면서 k-means클러스터링을 수행한다. 각 k의 값에 대하여 sse sum of squared error

