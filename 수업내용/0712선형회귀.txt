선형회귀

최소자승법
    모델의 파라미터를 구하기 위한 대표적인 방법 중 하나
    모델과 데이터와의 residual^2의 합 또는 평균을 최소화하도록 파라미터를 결정하는 방법
  활용
    (X^T)X가 존재할 때 문제의 해를 구할 수 있음
    데이터의 개수가 피쳐의 개수보다 많은 경우가 대부분이라서 자주 사용됨
    장점: 반복과 사용자가 지정하는 하이퍼파라미터가 존재하지 않아서 데이터만 있으면 쉽게 해를 구할 수 있음
    단점: 피쳐가 늘어나면 속도가 느려짐
        현재 컴퓨터의 연산속도로는 다른 알고리즘에 비해 느린 것이 아님

경사하강법
    경사를 하강하면서 수식을 최소화하는 매개변수의 값을 찾아내는 방법
    점이 최솟값을 달성하는 방향으로 점점 내려감
     - 몇 번 적용할 것인가?: 많이 실행할수록 최솟값에 가까워짐
     - 한 번에 얼마나 많이 내려갈 것인가?: 한 번에 얼마나 많은 공간을 움직일지를 기울기, 즉 경사라고 부름
    경사
     경사하강법의 하이퍼 매개변수
    수식, 원리 등이 너무 어렵다.
    아무튼 실제값, 예측값 간의 오차가 가장 낮은 값을 찾기 위해 사용한다..
    
    - 전체 배치(batch) 경사하강법
        모든 데이터를 한번에 입력하는 경사하강법
            *배치: 하나의 데이터셋
        하나의 값에 대한 경사도를 구한 다음 값들을 업데이트
        업데이트 횟수 감소: 가중치 업데이트 횟수가 줄어 계산상 효율성 상승
        안정적인 비용함수 수렴: 모든 값의 평균을 구하기 때문에 일반적으로 경사하강법이 갖는 지역 최적화 문제를 만날 가능성도 있음
        업데이트 속도 증가: 대규모 데이터셋을 한번에 처리하면 모델의 매개변수 업데이트 속도에 문제 발생이 적어짐
                데이터가 백만 단위 이상을 넘어가면 하나의 머신에서는 처리가 불가능해져서 메모리 문제가 발생
    
    - 확률적 경사하강법(SGD)
        학습용 데이터에서 샘플들을 랜덤하게 뽑아서 사용
        대상 데이터를 섞은 후 일반적인 경사하강법처럼 데이터를 한 개씩 추출하여 가중치를 구함
        +빈번한 업데이트, 데이터 분석가가 모델의 성능 변화를 빠르게 확인한다.
        +데이터 특성에 따라 훨씬 더 빠르게 결과값을 낸다.
        +지역최적화를 회피한다.
        -대용량 데이터를 사용하는 경우 시간이 매우 오래 걸림
        -결과의 마지막 값을 확인하기 어렵다.
        -튀는 현상.

    - 미니 배치 경사하강법
        데이터의 랜덤한 일부분만 입력해서 경사도 평균을 구해 가중치 업데이트
        에포크: 데이터를 한번에 모두 학습시키는 횟수
            전체배치SGD를 한번 학습하는 루프가 실행될 때 1에포크의 데이터가 학습된다고 말함
        배치사이즈: 한번에 학습되는 데이터의 개수
            총 데이터가 5012개 있고 배치 사이즈가 512라면 10번의 루프가 돌면서 1에포크를 학습했다고 말함.
        에포크, 배치사이즈는 하이퍼 매개변수, 데이터 분석가가 직접 선장한다.
훈련/테스트 분할
    머신러닝에서 데이터 학습을 하기 위한 학습 데이터셋과 학습의 결과로 생성된 모델의 성능을 평가하기 위한 테스트 데이터셋으로 나눔

모델이 데이터에 과다적합(오버피팅)된 경우
    생성된 모델이 특정 데이터에만 잘 맞아서 해당 데이터셋에 대해서는 성능을 발휘할 수 있지만 새로운 데이터셋에는 전혀 성능을 낼 수 없다.
모델이 데이터에 과소적합(언더피팅)된 경우
    기존 학습 데이터를 제대로 예측하지 못함

홀드아웃 메서드
    전체 데이터셋에서 일부를 학습 데이터와 테스트 데이터로 나누는 일반적인 데이터 분할 기법
        전체 데이터에서 랜덤하게 학습 데이터셋과 테스트 데이터셋을 나눔
        일반적으로 7:3 또는 8:2 정도의 비율
        sklearn 모듈이 제공하는 train_test_split 함수

선형회귀의 성능 측정 지표
    MAE(Mean Absolute Error) 평균 절대 잔차
    RMSE(Root Mean Squared Error) 평균 제곱근 오차
    결정계수(R-squared): 두 개의 값의 증감이 얼마나 일관성을 가지는지 나타내는 지표

과대적합(overfitting)
    머신러닝 모델을 학습할 때 학습 데이터셋에 지나치게 최적화해 발생하는 문제
    모델을 지나치게 복잡하게 학습해 학습 데이터셋에서는 모델 성능이 높게 나타나지만 
    정작 새로운 데이터가 주어졌을 때 정확한 예측/분류를 못함
과소적합(underfitting)
    과대적합의 반대 개념. 머신러닝 모델이 충분히 복잡하지 않아(최적화가 제대로 수행되지 않아)
    학습 데이터의 구조 및 패턴을 정확히 반영하지 못하는 문제

데이터의 편향, 분산
    편향은 한 쪽으로 쏠린 정도, 분산은 흩어진 정도
    편향:
        학습된 모델이 학습 데이터에 대해 만들어낸 예측값과 실제값의 차이
        모델의 결과가 얼마나 한쪽으로 쏠려있는지 나타냄
        편향이 크면 학습이 잘 진행되기는 하지만 해당 데이터에만 잘 맞음
    분산:
        학습된 모델이 테스팅 데이터에 대해 만들어낸 예측값과 실제값과의 차이
        모델의 결과가 얼마나 퍼져 있는지 나타냄

    편향-분산 트레이드오프:
        편향과 분산의 상충관계