RNN
RNN 쉘은 사장되는 중. 근데 LSTM등의 근간이 되기 때문에 알면 좋다.
순차데이터
    - 텍스트 ex) I am a boy
    - 시계열 ex) 1일 15도, 2일 17도, 3일 16도...
        시간 등의 연속적인 순서에 따라 데이터의 상관관계(증감)가 있는 데이터
    한번 전달이 아닌 연속적인 전달이 필요

    뉴런을 통과해 나온 결과(activation도 거치고)를 해당 뉴런에 새로운 입력과 함께 다시 통과한다.
        -순서를 기억하게 된 것.
        -그러나 자꾸 순환을 하며 값이 사라져버릴 수도 있다(activation을 거치며). 단기기억에 포커싱되어 있음.
    뉴런 자체가 순환해야하는 뉴런. 쉘. RNN뉴런.
    
    h activation function을 거친 값
    z activation function을 거치기 전의 결과 값

순환 신경망은 피드포워드 신경망과 매우 비슷하지만 뒤쪽으로 순환하는 연결도 있다는 점이 차이
벡터 투 시퀀스 네트워크: 각 타임 스텝에서 하나의 입력 벡터를 반복해서 네트워크에 주입하고 하나의 시퀀스를 출력
인코더 - 디코더

BPTT(backpropagation through time)
    RNN을 훈련하기 위한 기법은 타임 스텝으로 네트워크를 펼치고 보통의 역전파를 사용하는 전략
    정방향 패스는 다섯개의 입력 시퀀스가 주입된 다섯 번의 타임스텝을 말함
    순환 신경망은 일련의 타임 스텝을 진행하고 나서 그레디언트가 전파된다.

시계열: 타임스텝마다 하나 이상의 값을 가진 시퀀스
    단변량 시계열: 타임스텝이 하나. 웹사이트에서 시간당 접속 사용자의 수, 도시의 날자별 온도 등.
    다변량 시계열: 기업의 분기별 재정 안정성 지표(회사의 수입, 부채 등)처럼 타임스텝마다 여러 값이 존재.

텍스트 - 문장을 인식하기 위해 단어의 순서가 필요하고 단어를 인식하기 위해 숫자화가 필요.
    단어마다의 데이터를 연속적으로 넣어야 하므로 단어의 수만큼 타임스텝이 결정.(?)

RNN의 문제
    불안정한 그레디언트 문제
    학습이 지속되면 이전 기억의 소실
    과거 학습요소 상실
    단기 기억 문제

단기기억 문제 해결하기
    LSTM(장단기 메모리)셀
    핍홀 연결
    GRU(게이트 순환 유닛)셀
    1D 합성곱 층을 사용해 시퀀스 처리하기
    WAVENET


텍스트 전처리

말뭉치(코퍼스)
    말뭉치 또는 코퍼스(corpus, corpora)
    자연언어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집합
    자연어 처리 관련 애플리케이션은 방대한 양의 데이터
    코퍼스 분석 뿐만 아니라 언어 분석에서 사용되는 실제 언어의 체계적 디지털 모음
    둘 이상의 코퍼스가 있으면 코포라라고 부름
    코퍼스를 데이터세트라고도 함

    언어를 디지털화 한 것이 코퍼스

    단일 언어 코퍼스: 하나의 언어로 이루어짐
    이중 언어 코퍼스: 2개의 언어로 이루어짐
    다국어 코퍼스: 3개 이상의 언어로 이루어짐

텍스트 전처리
    자연어 처리에서 크롤링 등으로 얻어낸 코퍼스 데이터가 필요에 맞게 전처리되지 않은 상태시
    해당 데이터를 용도에 맞게 토큰화 & 정제 & 정규화를 진행해야함
    
    -토큰화: 용도에 맞게 자르는 것
        단어 토큰화
            토큰의 기준을 단어로 하여 토큰화하는 것
            단어는 단어 외에도 단어구, 의미를 갖는 문자열로도 간주됨
            보통 토큰화 작업은 단순히 구두점이나 특수 문자를 전부 제거하는 정제 작업을 수행하는 것만으로 해결되지 않음
            구두점이나 특수문자를 전부 제거하면 토큰이 의미를 잃어버리는 경우가 발생함
            띄어쓰기 단위로 자를시 단어 토큰 구분이 망가지는 언어도 존재
        문장 토큰화

        토큰화의 기준을 정할 때는 신중히.
        고려사항:
            구두점, 특수문자 등을 단순 제외해선 안된다.
                구두점조차도 하나의 토큰으로 분류하기도 한다.
                ex) 마침표(.) - 문장의 경계
                    단어 자체에 구두점 - m.p.h, Ph.D, AT&T, $1000 등
            줄임말과 단어 내에 띄어쓰기가 있는 경우
                ex) he's, New York
        penn treebank
    
    -정제: 컴퓨터가 알아먹도록 하는 것
    -정규화: 정제된 값을 정규화

*텍스트 전처리는 항상 순차적으로 진행되는 건 아님.(사실 일반 데이터 또한 그렇긴 하다.)